\begin{MintedVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import}\PYG{+w}{ }\PYG{n+nn}{torch}

\PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{lerobot}\PYG{n+nn}{.}\PYG{n+nn}{datasets}\PYG{n+nn}{.}\PYG{n+nn}{lerobot\PYGZus{}dataset}\PYG{+w}{ }\PYG{k+kn}{import} \PYG{n}{LeRobotDataset}
\PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{lerobot}\PYG{n+nn}{.}\PYG{n+nn}{policies}\PYG{n+nn}{.}\PYG{n+nn}{factory}\PYG{+w}{ }\PYG{k+kn}{import} \PYG{n}{make\PYGZus{}policy}\PYG{p}{,} \PYG{n}{make\PYGZus{}pre\PYGZus{}post\PYGZus{}processors}
\PYG{k+kn}{from}\PYG{+w}{ }\PYG{n+nn}{lerobot}\PYG{n+nn}{.}\PYG{n+nn}{policies}\PYG{n+nn}{.}\PYG{n+nn}{sac}\PYG{n+nn}{.}\PYG{n+nn}{reward\PYGZus{}model}\PYG{n+nn}{.}\PYG{n+nn}{configuration\PYGZus{}classifier}\PYG{+w}{ }\PYG{k+kn}{import} \PYG{n}{RewardClassifierConfig}

\PYG{c+c1}{\PYGZsh{} Device to use for training}
\PYG{n}{device} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mps}\PYG{l+s+s2}{\PYGZdq{}}  \PYG{c+c1}{\PYGZsh{} or \PYGZdq{}cuda\PYGZdq{}, or \PYGZdq{}cpu\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} Load the dataset used for training}
\PYG{n}{repo\PYGZus{}id} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lerobot/example\PYGZus{}hil\PYGZus{}serl\PYGZus{}dataset}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{LeRobotDataset}\PYG{p}{(}\PYG{n}{repo\PYGZus{}id}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Configure the policy to extract features from the image frames}
\PYG{n}{camera\PYGZus{}keys} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{meta}\PYG{o}{.}\PYG{n}{camera\PYGZus{}keys}

\PYG{n}{config} \PYG{o}{=} \PYG{n}{RewardClassifierConfig}\PYG{p}{(}
    \PYG{n}{num\PYGZus{}cameras}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{camera\PYGZus{}keys}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{device}\PYG{o}{=}\PYG{n}{device}\PYG{p}{,}
    \PYG{c+c1}{\PYGZsh{} backbone model to extract features from the image frames}
    \PYG{n}{model\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{microsoft/resnet\PYGZhy{}18}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Make policy, preprocessor, and optimizer}
\PYG{n}{policy} \PYG{o}{=} \PYG{n}{make\PYGZus{}policy}\PYG{p}{(}\PYG{n}{config}\PYG{p}{,} \PYG{n}{ds\PYGZus{}meta}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{meta}\PYG{p}{)}
\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{config}\PYG{o}{.}\PYG{n}{get\PYGZus{}optimizer\PYGZus{}preset}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{build}\PYG{p}{(}\PYG{n}{policy}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{preprocessor}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{make\PYGZus{}pre\PYGZus{}post\PYGZus{}processors}\PYG{p}{(}\PYG{n}{policy\PYGZus{}cfg}\PYG{o}{=}\PYG{n}{config}\PYG{p}{,} \PYG{n}{dataset\PYGZus{}stats}\PYG{o}{=}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{meta}\PYG{o}{.}\PYG{n}{stats}\PYG{p}{)}


\PYG{n}{classifier\PYGZus{}id} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lerobot/reward\PYGZus{}classifier\PYGZus{}hil\PYGZus{}serl\PYGZus{}example}\PYG{l+s+s2}{\PYGZdq{}}  \PYG{c+c1}{\PYGZsh{} your HF username and model repo id for the reward classifier}

\PYG{c+c1}{\PYGZsh{} Instantiate a dataloader}
\PYG{n}{dataloader} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{utils}\PYG{o}{.}\PYG{n}{data}\PYG{o}{.}\PYG{n}{DataLoader}\PYG{p}{(}\PYG{n}{dataset}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{,} \PYG{n}{shuffle}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Training loop}
\PYG{n}{num\PYGZus{}epochs} \PYG{o}{=} \PYG{l+m+mi}{5}
\PYG{k}{for} \PYG{n}{epoch} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num\PYGZus{}epochs}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{total\PYGZus{}loss} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{n}{total\PYGZus{}accuracy} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{k}{for} \PYG{n}{batch} \PYG{o+ow}{in} \PYG{n}{dataloader}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} Preprocess the batch and move it to the correct device.}
        \PYG{n}{batch} \PYG{o}{=} \PYG{n}{preprocessor}\PYG{p}{(}\PYG{n}{batch}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Forward pass}
        \PYG{n}{loss}\PYG{p}{,} \PYG{n}{output\PYGZus{}dict} \PYG{o}{=} \PYG{n}{policy}\PYG{o}{.}\PYG{n}{forward}\PYG{p}{(}\PYG{n}{batch}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Backward pass and optimization}
        \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{zero\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{loss}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{p}{)}

        \PYG{n}{total\PYGZus{}loss} \PYG{o}{+}\PYG{o}{=} \PYG{n}{loss}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{total\PYGZus{}accuracy} \PYG{o}{+}\PYG{o}{=} \PYG{n}{output\PYGZus{}dict}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{accuracy}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}

    \PYG{n}{avg\PYGZus{}loss} \PYG{o}{=} \PYG{n}{total\PYGZus{}loss} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{dataloader}\PYG{p}{)}
    \PYG{n}{avg\PYGZus{}accuracy} \PYG{o}{=} \PYG{n}{total\PYGZus{}accuracy} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{dataloader}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Epoch }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{epoch}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{/}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{num\PYGZus{}epochs}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Loss: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{avg\PYGZus{}loss}\PYG{l+s+si}{:}\PYG{l+s+s2}{.4f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Accuracy: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{avg\PYGZus{}accuracy}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Training finished!}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} You can now save the trained policy.}
\PYG{n}{policy}\PYG{o}{.}\PYG{n}{push\PYGZus{}to\PYGZus{}hub}\PYG{p}{(}\PYG{n}{classifier\PYGZus{}id}\PYG{p}{)}
\end{MintedVerbatim}
