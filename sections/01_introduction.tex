\section{Introduction}

\subsection{Motivation: The Interdisciplinary Nature of Robotics in the Machine Learning Era}
Robotics in 2025 sits at the intersection of classical model-based control, machine perception, and large-scale machine learning. Foundational problems in locomotion, manipulation, and whole-body control demand reasoning across rigid-body dynamics, contact modeling, planning under uncertainty, and high-dimensional function approximation. At the same time, end-to-end learning has matured from proof-of-concept demonstrations to systems that benefit from internet-scale multimodal pretraining and robotics-specific fine-tuning, closing the gap between laboratory benchmarks and deployment in unstructured settings. This tutorial embraces that interdisciplinarity: it treats modern robot learning as a synthesis of classical priors and data-driven policies rather than a replacement of one by the other.

\subsection{Scope and Contributions of This Work}
This document serves two purposes. First, it provides a concise, academically rigorous overview of core concepts in classical robotics that remain essential for principled system design (kinematics, dynamics, planning, and control). Second, it surveys contemporary learning-based methods for robotic control, with an emphasis on (i) reinforcement learning (RL) and imitation learning (IL), (ii) single-task policy architectures such as transformer-based action chunking and diffusion policies, and (iii) multi-task vision–language–action (VLA) models. Throughout, we complement exposition with \emph{reproducible} implementation guidance using the open-source \texttt{LeRobot} framework, to lower the barrier between concept and practice while maintaining scientific rigor.

\subsection{Structure of the Report}
Section~\ref{sec:classical} reviews classical robotics foundations and their limitations in contact-rich, high-DOF regimes. Section~\ref{sec:learning} formulates learning-based control via RL and IL, highlighting problem setups, algorithms, and known failure modes (e.g., reward misspecification, simulation gaps, and safety constraints). Section~\ref{sec:single} details single-task policy families (transformer chunking, diffusion) and their practical training recipes. Section~\ref{sec:multi} surveys multi-task VLA models (e.g., RT-1/RT-2, OpenVLA, $\pi_0/\pi_{0.5}$, and SmolVLA), together with integration patterns and experimental evaluation protocols. Section~6, not covered here, discusses emerging directions (e.g., world models and post-training) beyond the present scope.