\section{Related work}

\paragraph{Vision-language models (VLMs).} VLMs are designed to process both visual and textual modalities--most commonly by taking both images and text as input and generating text conditioned on the visual context. 
Recent advances in VLMs have been driven by the success of LLMs, with many approaches building upon pretrained LLMs and adopting similar training paradigms. 
Typically, VLMs~\citep{alayrac2022flamingo,laurenccon2024mattersidefics2,VILA} are constructed by integrating a pretrained vision encoder~\citep{radford2021learning,li2023siglip,fini2024multimodalaimv2} with a pretrained LLM~\citep{llama3_2modelcard,Mistral7B,wang2024qwen2}. 
Training then proceeds in multiple multimodal stages, beginning with large-scale pretraining on image-caption datasets~\citep{LAION-COCO,COYO-700M} and interleaved vision-language corpora~\citep{OBELICS,MMC4}, all followed by a supervised fine-tuning stage on instruction-tuning datasets~\citep{LLaVA-1.5,tong2024cambrian,laurenccon2024mattersidefics2}. 
Other works have shown the benefits of not relying on pretrained vision encoders~\citep{fuyu-8b,shukor2025scaling,diao2025evev2,diao2024unveiling}, while other works aims at developing more unified architectures representing both images and text as discrete tokens, enabling a single model to process multimodal sequences of tokens~\citep{wang2022ofa,shukor2023unival,team2024chameleon,lin2024moma}. 
Efficiency has also become a central focus in VLM research. Several works aim to reduce training costs by using smaller, more diverse datasets~\citep{LLaVA-1.5,InstructBLIP,bai2025qwen25vl,zhu2024minigpt,tong2024cambrian}, training smaller-scale models~\citep{marafioti2025smolvlm,moondream,minicmpv2024}, or by adapting pretrained unimodal models by tuning only a small subset of parameters~\citep{shukor2023epalm,vallaeys2024improveddepalm,MAPL,FROMAGe,tsimpoukelli2021multimodalfrozen,BLIP-2}. 
While the majority of VLM research focuses on image and text modalities, recent work has demonstrated that similar techniques can be extended to integrate additional modalities, such as video and audio~\citep{wang2025internvideo2,liu2024kangaroo,zhang2025videollama,kong2024audioflam}.


\paragraph{Vision-language-action models (VLAs).} A growing area of interest in robotics research is the development of generalist policies--models capable of performing a wide range of tasks, generalizing across different environments and robot embodiments. 
A prominent strategy in this direction leverages VLAs, models capable of processing \emph{(i)} task instructions given in natural language, \emph{(ii)} visual observations (e.g., images coming from camera streams), and \emph{(iii)} proprioceptive inputs to output control actions.
Early efforts such as Octo~\citep{team2024octo} and RT-1~\citep{o2024openrtx} trained transformer-based models from scratch on large-scale robotic demonstration datasets. To improve both performance and generalization, RT-2~\citep{brohan2023rt2} leveraged pretrained vision-language models (VLMs), further training them on robotics-specific data. 
In an effort to promote openness and reproducibility, OpenVLA~\citep{kimopenvla} released a 7B-parameter VLA trained on publicly available data to generate discrete action tokens. 
As action tokenization poses limitations for continuous control, \( \pi_0 \) \citep{black2024pi_0} and DexVLA \citep{wen2025dexvla} proposed using diffusion-based decoders for continuous action generation. 
In this, both~\citet{black2024pi_0, wen2025dexvla} propose adapting a pretrained VLM, RDT-1B,introducing a large diffusion component--termed \emph{action expert}--trained directly on robotic demonstrations. 
Recently,~\cite{pertsch2025fast} proposed a fully autoregressive approach using a novel action tokenizer, improving over traditional binning methods but still suffering from slow (autoregressive) inference. 
In an effort to improve VLAs' efficiency, TinyVLA~\citep{wen2024tinyvla} trained a lightweight sub-1B model from scratch on multimodal data and then fine-tuned it on robotics datasets, although the lack of large-scale pretraining on robotics data hinders wider generalization capabilities. 
\ours shares similar goals with most of these efforts, aiming to develop and release open-source models that are both performant and highly efficient in terms of training and inference.

% generalist policies like octo and rt1 trained on open-x.
% common transformer with vision encoders
% then leveragin vlms rt2 and openvla howver discrete

% pi0 dexvla and rdt leverage vlms and diffusion or flow matching and pretrained on 

% fast autoregressive but slow 

% tinyvla same spirit as ours but no pretraiing which limits the generalization and rely on weak vlm and private data?

% \citep{o2024openrtx} rtx: model + openx data show positive transfer. they train rt1 and rt2 on their data.

% \citep{brohan2022rt1} rt-1: 35M model, with a transformer and tokenized actions with ce loss. not vla 

% \cite{team2024octo} octo  diffusion head trained on open-x embodiment, not based on vlms, s and vit-b scale. 2 frames history of observations. not vla?



% \citep{brohan2023rt2} rt2: trained on both multimodal and robotics data. actions as text tokens. continue training of vlms from 3b to 55b sizes. and slower than 5 hz even on tpus. trained on webli and openx. significantly better than rt1 so vla is better 
% tokenize actions 



% \cite{kimopenvla} openvla discrete tokens: 7b param finetune vlm to generate actions. open-source with lora . run less than 5 hz with a quantized version. trained on open-x.


% \cite{black2024pi_0} pi0: similar to our work with flow matching. differences? train on mix of open and private data. Big model more than 3B params. we differ in architecture

% \cite{pertsch2025fast} pi0fast: autoregressive with new tokenization. fast at training but slow at inference.

% \cite{liu2024rdt} rdt-1b: diffusion based. 1.2b, important unified action space. fast at inference. fully open-source not based on a vlm 

% \cite{wen2025dexvla} dexvla: similar to octo. diffusion expert. trained to generation actions and text tokens. 1b expert. uses multiple heads for cross-embodiment. 2b vlm + 1b diffusion. 60hz in nvidia gpu. private data.   

% efficient: 
% \cite{wen2024tinyvla} tinyvla: no pretraining on robot datasets. S, base and large. they train their own vlm and then ft on robotics data. diffusion policy. 100 traj per task 5 tasks.


% \paragraph{Efficient foundation models.}