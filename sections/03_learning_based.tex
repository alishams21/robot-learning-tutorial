\section{Robot Learning}
\label{sec:learning}

\epigraph{\textit{Approximate the solution, not the problem} [...]}{Richard Sutton}

\begin{tldr}
The need for expensive high-fidelity simulators can be obviated by learning from real-world data, using sample-efficient algorithms that can safely train directly on hardware.
\end{tldr}

Learning-based techniques for robotics naturally address the limitations presented in~\ref{sec:classical} (Figure~\ref{fig:robot-learning-upsides}).
Learning-based techniques typically rely on prediction-to-action (\emph{visuomotor policies}), thereby directly mapping sensorimotor inputs to predicted actions, streamlining the harmonization of control policies.
Mapping sensorimotor inputs to actions directly also allows to add diverse input modalities over time, leveraging the automatic feature extraction characteristic of most common learning systems.
Further, learning-based approaches can in principle entirely bypass modeling efforts and instead rely exclusively on interactions data, proving transformative when dynamics are challenging to model or unknown.
Lastly, we argue learning for robotics (\emph{robot learning}) is naturally well posed to leverage the growing amount of robotics data openly available, just as computer vision first and natural language processing later did historically benefit from large scale corpora of (possibly non curated) data.
As it largely leverages techniques and results developed within more mature subfields within ML (computer vision-based feature extractors, language-based semantics, etc.), robot learning is thus posed to potentially benefit from the growing amount of motion data available, in great part overlooked by dynamics-based approaches.

Robot learning is a field at its relative nascent stages, and no prevalent technique(s) proved distinctly better than others.
Still, two major classes of methods gained prominence in applications of ML to robotics: \highlight{reinforcement learning (RL)} and \highlight{behavioral cloning (BC)}.
In this section, we provide a conceptual overview of applications of the former to robotics, as well as introduce practical examples of how to use RL within \lerobot.
We then introduce the major limitations RL suffers from, introducing BC shortly after.
Importantly, (Figure~\ref{fig:robot-learning-atlas}) we decided to include generalist robot models~\citep{black$p_0$VisionLanguageActionFlow2024,shukorSmolVLAVisionLanguageActionModel2025} alongside task-specific BC methods.
While very different in spirit---generalist models learn to take instructions as input and to use them to generate motion valid across many tasks---foundation models like \( \pi_0 \), for instance, are largely trained to reproduce trajectories contained in a large training set of input demonstrations.
This distinction being made, we argue generalist policies can indeed be grouped alongside other BC methods.

Figure~\ref{fig:robot-learning-atlas} illustrates this classification graphically, explicitly listing all the robot learning policies currently available in \lerobot: Action Chunking with Transformers (ACT)~\citep{zhaoLearningFineGrainedBimanual2023}, Diffusion Policy~\citep{chiDiffusionPolicyVisuomotor2024}, Vector-Quantized Behavior Transformer (VQ-BeT)~\citep{leeBehaviorGenerationLatent2024}, \( \pi_0 \)~\citep{black$p_0$VisionLanguageActionFlow2024}, SmolVLA~\citep{shukorSmolVLAVisionLanguageActionModel2025}, Human-in-the-loop Sample-efficient RL (HIL-SERL)~\citep{luoPreciseDexterousRobotic2024} and TD-MPC~\citep{hansenTemporalDifferenceLearning2022}.

\subsection{RL for Robotics}
Applications of RL to robotics have been long studied, to the point the relationship between these two disciplines has been compared to that between physics and matematics~\citep{koberReinforcementLearningRobotics}.
Indeed, due to their interactive and sequential nature, many robotics problems can be directly mapped to RL problems.
Figure~\ref{fig:robotics-with-rl-examples} depicts two of such cases. Manipulating a tiling belt into its correct position is a sequential problem where at each cycle the controller needs to adjust the position of the robotic arms based on their current configuration and the state of the tiling belt. 
In this example, sequentiality arises from the impossibility of placing the belt in its goal position right away from any starting position.
Figure~\ref{fig:robotics-with-rl-examples} also shows an example of a locomotion problem, where sequentiality is inherent in the problem formulation. Sliding to the side, the controller has to constantly keeps adjusting to the robot's propioperception, avoiding failure (falling).

\subsubsection{A (Concise) Introduction to RL}
The RL framework~\citep{suttonReinforcementLearningIntroduction2018}, which we briefly introduce here, has often been used to model robotics problems~\citep{koberReinforcementLearningRobotics}.
RL is a subfield within ML fundamentally concerned with the development of autonomous systems (\emph{agents}) learning how to \emph{continuously behave} in an evolving environment, developing control \emph{policies} to better act when in a given situation.
Crucially for robotics, agents improve via trial-and-error only, entirely bypassing the need to develop explicit models of the problem dynamics, and rather exploiting interaction data only.
In RL, this feedback loop between actions and outcomes is established through the agent sensing a scalar quantity (\emph{reward}, Figure~\ref{fig:rl-most-famous-pic}).

Formally, interactions between an agent and its environment are typically modeled via a Markov Decision Process (MDP).
Representing robotics problems via MDPs offers several advantages, including (1) incorporating uncertainty through MDP's inherently stochastic nature and (2) providing a theoretically sound framework for learning without an explicit dynamic model.
While accommodating also a continuous time formulation, MDPs are typically considered in discrete time in RL, considering interactions to atomically take place over the course of discrete \emph{timestep} \( t=0,1,2,3, \dots, T \).
MDPs allowing for an unbounded number of interactions ( \( T \to + \infty \) ) are typically termed \emph{infinite-horizon}, and opposed to \emph{finite-horizon} MDPs in which \( T \) cannot grow unbounded.
Unless diversely specified, we will only be referring to discrete-time episodic MDPs.


Formally, a lenght-\(T\) Markov Decision Process (MDP) is a tuple \( \mathcal M = \langle \statespace, \actionspace, \dynamics, r, \gamma, \rho, T \rangle \), where:
\begin{itemize}
    \item \(\statespace\) is the \emph{state space}; \(\state \in \statespace\) denotes the (possibly non-directly observable) environment state at time \(t\). In robotics, states often comprise robot configuration and velocities (\(q_t, \dot q_t\)), and can accomodate sensor readings such as camera or audio streams.
    \item \(\actionspace\) is the \emph{action space}; \(\action \in \actionspace\) may represent joint torques, joint velocities, or even end-effector commands. In general, actions correspond to commands intervenings on the configuration of the robot. 
    \item \(\dynamics\) represents the (possibly non-deterministic) environment dynamics, with \(\dynamics: \statespace \times \actionspace \times \statespace \mapsto [0, 1] \) corresponding to \( \dynamics \, \transition = \transitionprob \). For instance, for a planar manipulator dynamics can typically be considered deterministic when the environment is fully described (Figure~\ref{fig:planar-manipulation-simple}), and stochastic when disturbances depending on non-observable parameters intervene (Figure~\ref{fig:planar-manipulator-box-velocity}).
    \item \(r: \statespace \times \actionspace \times \statespace \to \mathbb R\) is the \emph{reward function}, weighing \( \transition \) in the context of an arbitrary goal. For instance, a simple reward function for quickly moving the along the \( x \) axis in 3D-space could be based on the position of the robot along the \( x \) axis~(\(p_x\)), present negative penalties for falling over (measured from \( p_z \)) and a introduce bonuses \( \dot p_x \) for speed.
\end{itemize}
Lastly, \(\gamma \in [0,1)\) represent the discount factor regulating preference for immediate versus long-term reward (with a horizon of \( \tfrac{1}{1-\gamma} \)), and \( \rho \) is the distribution defined over \(\statespace, \) to sample the initial state, \( s_0 \sim \rho \).

For MDPs, a \emph{trajectory} of length \(T\) is the (random) sequence
\begin{equation}\label{eq:trajectory_definition}
    \tau = \trajectory,
\end{equation}
with per-step rewards defined as \(r_t = r \transition \) for ease of notation.

Interestingly, assuming both the environment dynamics and conditional distribution over actions given states---termed \emph{policy}---to be \emph{Markovian}:
%
\begin{align}
\mathbb P(\stateplusone \vert s_t, a_t, s_{t-1}, a_{t-1}, \dots s_0, a_0 ) &= \mathbb P \transitiongiven \label{eq:dynamics_markovian} \\
\mathbb P(\action \vert \state, a_{t-1}, s_{t-1}, s_0, a_0) &= \mathbb P(\action \vert \state) \label{eq:policy_markovian}
\end{align}
%
The probability of observing a given trajectory \( \tau \) can be piece-wise factorized into
\begin{equation}\label{eq:traj_prob}
    \mathbb P(\tau) = \mathbb P (s_0) \prod_{t=0}^{T-1} \mathbb P \transitiongiven \ \mathbb P(\action \vert \state).
\end{equation}

Policies \( \mathbb P(\action \vert \state) \) are typically indicated as \( \pi(\action \vert \state) \), and often parametrized via \( \theta \), yielding \( \pi_\theta (\action \vert \state )\).
Thus, one can define the (discounted) \emph{return} associated to \( \tau \) as the (random) sum of measured rewards,
\[
    G(\tau) = \sum_{t=0}^{T-1} \gamma^{t} r_t.
\]
Agents seek to learn control strategies (\emph{policies}, \( \pi_\theta \)) maximizing the expected return \( \mathbb E G(\tau) \). 
For a given dynamics \( \mathcal D \)---i.e., for a given problem---taking the expectation over the (possibly random) trajectories resulting from acting according to a certain policy provides a direct, goal-conditioned ordering of all the possible policies obtained, yielding the (maximization) target \( J : \Pi \mapsto \mathbb R \)
\begin{align}
    J(\pi_\theta) &= \mathbb E_{\tau \sim \mathbb P_{\theta; \mathcal D}} \left[ G(\tau) \right], \label{eq:RL-j-function} \\
    \mathbb P_{\theta; \mathcal D} (\tau) &= \rho \prod_{t=0}^{T-1} \mathcal D \transition \ \pi_\theta (\action \vert \state).\label{eq:traj-probabilities-for-policies}
\end{align}

MDPs thus naturally provide a framework to optimize over the space of the possible behaviors an agent might enact (\( \pi \in \Pi \)), searching for the \emph{optimal policy} \( \pi^* = \arg \max_{\theta} J(\pi_\theta) \).
Providing a target for policy search, \( G(\tau) \) can also be used as a target to discriminate between states and state-action pairs.
Given any state \( s \in \statespace \)---e.g., a given configuration of the robot---the \emph{state-value} function
\[
    V_\pi(s) = \mathbb E_{\tau \sim \pi} \left[ G(\tau) \big \vert s_0 = s \right]
\]
can be used to discriminate between desirable and undesirable state in terms of long-term (discounted) reward maximization, under a given policy \(\pi\).
Similarily, the \emph{state-action} value function conditions the cumulative discounted reward also on selecting action \( a \) in \( s \), and thereafter act according to \( \pi \):
\[
    Q_\pi(s,a) = \mathbb E_{\tau \sim \pi} \left[ G (\tau) \big \vert s_0 = s, a_0=a \right]
\]
Crucially, value functions are interrelated:
\begin{align}
Q_\pi(s_t, a_t) &= \mathbb{E}_{\stateplusone \sim \mathbb P(\bullet \vert \state, \action)} \left[ r_t + \gamma V_\pi(\stateplusone) \right] \label{eq:q-as-v} \\
V_\pi(\state) &= \mathbb E_{\action \sim \pi(\bullet \vert \state)} \left[ Q_\pi (\state, \action) \right]
\label{eq:v-as-q}
\end{align}
They also induce an ordering over states and state-action pairs under \( \pi \) and are thus central to most RL algorithms.
A variety of methods have been developed in RL as standalone attemps to find (approximate) solutions to the problem of maximizing cumulative reward (Figure~\ref{fig:rl-algos-atlas}).
Popular approaches to continuous state and action space---such as those studied within robotics---include~\citet{schulmanTrustRegionPolicy2017, schulmanProximalPolicyOptimization2017, haarnojaSoftActorCriticOffPolicy2018}.
Across manipulation~\citep{akkayaSolvingRubiksCube2019} and locomotion~\citep{leeLearningQuadrupedalLocomotion2020} problems, RL proved extremely effective in providing a platform to (1) adopt a unified, streamlined perception-to-action pipeline, (2) natively integrate propioperception with multi-modal high-dimensional sensor streams  (3) disregard a description of the environment dynamics, by focusing on observed interaction data rather than modeling, and (4) anchor policies in the experience collected and stored in datasets.
For a more complete survey of applications of RL to robotics, we refer the reader to~\citet{koberReinforcementLearningRobotics,tangDeepReinforcementLearning2024}.

\subsubsection{Real-world RL for Robotics}
Streamlined end-to-end control pipelines, data-driven feature extraction and a disregard for explicit modeling in favor of interaction data are all features of RL for robotics.
However, particularly in the context of real-world robotics RL still suffers from limitations concerning machine safety and learning efficiency.

First, especially early in training, \highlight{actions are typically explorative, and thus erractic}.
On physical systems, untrained policies may command high velocities, self-collisiding configurations, or torques exceeding joint limits, leading to wear and potential hardware damage.
Mitigating these risks requires external safeguards (e.g., watchdogs, safety monitors, emergency stops), and often even human supervision.
Further, in the typical episodic setting considered in most robotics problems, experimentation is substantially slowed down by the need to manually reset the environment over the course of training, a time-consuming and brittle process.

Second, learning with a limited number of samples remains problematic in RL, \highlight{limiting the applicability of RL in real-world robotics due to consequently prohibitive timescales of training}.
Even strong algorithms such as SAC~\citep{haarnojaSoftActorCriticOffPolicy2018} typically require a large numbers of transitions \( \{ \sars \}_{t=1}^N \).
On hardware, generating these data is time-consuming and demands reliable resetting to initial conditions, which is itself a non-trivial problem.

Critically, training RL policies in simulation~\citep{tobinDomainRandomizationTransferring2017} addresses both issues: it eliminates physical risk and dramatically increases throughput. 
Yet, simulators require significant modeling effort, and rely on assumptions (simplified physical modeling, instantaneous actuation, static environmental conditions, etc.) limiting transferring policies learned in simulation due the discrepancy between real and simulated environments (\emph{reality gap}, Figure~\ref{fig:synthetic-vs-real-duck}).
\emph{Domain randomization} (DR) is a popular technique to overcome the reality gap, consisting in randomizing parameters of the simulated environment during training, to induce robustness to specific disturbances.
In turn, DR is employed to increase the diversity of situations observed over the course the training, improving on the chances sim-to-real transfer~\citep{akkayaSolvingRubiksCube2019,antonovaReinforcementLearningPivoting2017,jiDribbleBotDynamicLegged2023}.
In practice, DR is performed further parametrizing the \emph{simulator}'s dynamics \( \mathcal D \equiv \mathcal D_\xi \) with a \emph{dynamics} (random) vector \( \xi \) drawn an arbitrary distribution, \( \xi \sim \Xi \).
Over the course of training---typically at each episode's reset---a new \( \xi \) is drawn, and used to specify the environment's dynamics for that episode.
For instance, one could decide to randomize the friction coefficient of the surface in a locomotion task (Figure~\ref{fig:ducks-on-terrains}), or the center of mass of an object for a manipulation task.

While effective in transfering policies across the reality gap~\citep{jiDribbleBotDynamicLegged2023,tiboniDomainRandomizationEntropy2024}, DR often requires extensive manual engineering.
First, identifying which parameters to randomize---i.e., the \emph{support} \( \text{supp} \Xi) \) of \( \Xi \)---is an inherently task specific process.
When locomoting over different terrains, choosing to randomize over the friction coefficient is a reasonable choice, yet not completely resolutive as other factors (lightning conditions, external temperature, joints' fatigue, etc.) may prove just as important, making selecting these parameters another source of brittlness.
Further, selecting the dynamics distribution \( \Xi \) is also non-trivial.
On the one hand, distributions with low entropy might risk to cause failure at transfer time, due to the limited robustness induced over the course of training.
On the other hand, excessive randomization may cause over-regularization and hinder performance.
Consequently, the research community started investigating approaches to automatically select the randomization distribution using signals from the training process.
~\citet{akkayaSolvingRubiksCube2019} use a parametric uniform distribution \( \mathcal U(a, b) \) as \( \Xi \), widening the bounds as training progresses and agent's performance improves (AutoDR). 
While effective, AutoDR requires significant tuning, and even disregard data when performance \emph{does not} improve after a distribution update~\citep{tiboniDomainRandomizationEntropy2024}.
~\citet{tiboniDomainRandomizationEntropy2024} propose a different method (DORAEMON) to evolve \( \Xi \) based on the success rate (tracked during training), explicitly maximizing parametric Beta distribution, inherently more flexible than uniform distributions.
DORAEMON proves particularly effective at dynamically increasing the entropy levels of the training distribution employing a max-entropy under performance constraints formulation, differently from AutoDR, which updates a possibly-multivariate environment distributions with fixed, scalar increments on individual bounds.
Other approaches to automatic DR consist in specifically tuning \( \Xi \) to align as much as possible the simulation and real-world domains.
For instance, ~\citet{chebotar2019closing} interleave in-simulation policy training with repeated real-world policy rollouts used to adjust \( \Xi \) based on real-world data, while ~\citet{tiboniDROPOSimtoRealTransfer2023} leverage a single, pre-collected set of real-world trajectories and tune \( \Xi \) under a simple likelihood objective.

DR has shown promise, but critically, even under the assumption that an ideal distribution \( \Xi \) to sample from was indeed available, many robotics applications cannot be simulated with high-enough fidelity under practical computational constraints in the first place.
Simulating contact-rich manipulation of possibly deformable or soft materials---i.e., \emph{folding a piece of clothing}---can be costly and even time-intensive, limiting the benefits of in-simulation training.
An even more important problem is the general unavailability of complicated tasks' \emph{dense} reward function, the design of which is essentially based on human expertise and trial-and-error.
In practice, \emph{sparse} reward functions are used much more to conclude whether one specific goal has been attained---\emph{is this t-shirt correctly folded?}---but unfortunately incur in more complicated learning.
As a result, despite notable successes, deploying RL directly on real-world robots at scale remains challenging.

\subsubsection{RL in \lerobot: sample-efficient, data-driven, and real-world}

To make the most of (1) the growing number of openly available datasets and (2) relatively inexpensive robots like the SO-100, RL should be anchored in already-collected trajectories and train in the real-world directly.
Crucially, methods designed for accessible robot platforms (e.g., the SO-100) could in principle be transferred to higher-end robots (e.g., Franka Emika Panda), provided they are general enough.
In this context, sample-efficient learning is also paramount, as training on the real-world is inherently time-bottlenecked.

Off-policy algorithms like Soft Actor-Critic (SAC)~\citep{haarnojaSoftActorCriticOffPolicy2018} tend to be more sample efficient then their on-policy counterpart~\citep{schulmanProximalPolicyOptimization2017}, due to the presence a \emph{replay buffer} used over the course of the training.
Other than allowing to re-use transitions \( \sars \) over the course of training, the replay buffer can also accomodate for the injection of previously-collected data in the training process~\citep{ballEfficientOnlineReinforcement2023}.
Using expert demonstrations to guide learning, and learning-based reward RL can effectively be deployed in the real-world~\citep{luoSERLSoftwareSuite2025}.
Interestingly, when completed with in-training human interventions, real-world RL agents learn policies with near-perfect success rates on challenging manipulation tasks in 1-2 hours~\citep{luoPreciseDexterousRobotic2024}.

% DQN to DDPG to SAC
In an MDP, the optimal policy \( \pi^* \) can be derived from its associated \qfunction, \( Q_{\pi^*} \), and in particular the optimal action(s) \(\mu(\state)\) can be selected maximizing the optimal \qfunction over the action space,
\[
\mu(\state) = \max_{\action \in \mathcal A} Q_{\pi^*}(\state, \action).
\]
Interestingly, the \qopt function satisfies a recursive relationship (\emph{Bellman equation}) based on a very natural intuition%
\footnote{Quote from~\citet{mnihPlayingAtariDeep2013}. Slightly adapted for consistency of notation throughout this work.}:
\begin{quote}
    [...] If the optimal value \( Q^*(\stateplusone, a_{t+1}) \) of the [state] \(\stateplusone \) was known for all possible actions \(a_{t+1} \), then the optimal strategy is to select the action \( a_{t+1}\) maximizing the expected value of \( r_t + \gamma Q^*(s_{t+1}, a_{t+1}) \)
\[ 
Q^*(s_t, a_t) = \mathbb E_{s_{t+1} \sim \mathbb P(\bullet \vert s_t, a_t)} \left[ r_t + \gamma \max_{a_{t+1} \in \mathcal A} Q^*(s_{t+1}, a_{t+1}) \big\vert s_t, a_t  \right]
\]
\end{quote}

In turn, the optimal \qfunction \ %
is guaranteed to be self-consistent by definition.
\emph{Value-iteration} methods exploit this relationship (and/or its state-value counterpart, \( V^*(s_t) \) ) by iteratively updating an initial estimate of \qopt, \( Q_k \) using the Bellman equation as update rule (\emph{Q-learning}):
\[
    Q_{i+1}(s_t, a_t) \leftarrow \mathbb E_{s_{t+1} \sim \mathbb P(\bullet \vert s_t, a_t)} \left[ r_t + \gamma \max_{a_{t+1} \in \mathcal A} Q_i (s_{t+1}, a_{t+1}) \big\vert s_t, a_t  \right],  \quad i=0,1,2,\dots,K
\]
Then, one can derive the (ideally) near-optimal policy by explicitly maximizing over the action space the final the (ideally) near-optimal estimate \( Q_K \approx Q^* \) over the action space.

Effective in its early applications to small-scale discrete problems and theoretically sound (\( Q_K \approx Q^* \, \text{as } K \to \infty \)), vanilla Q-learning was found complicated to scale to large \( \statespace \times \actionspace \) problems, in which storing alone \( Q : \statespace \times \actionspace \mapsto \mathbb R \) might result prohibitive, as well as not directly usable for continuous state-action space MPDs, such as those considered in robotics.
In their seminal work on \emph{Deep Q-Learning} (DQN),~\citet{mnihPlayingAtariDeep2013} propose learning Q-values using deep convolutional neural networks, parametrizing Q-functions using neural networks with parameters \( \theta \), updated by sequentially minimizing the temporal-difference (TD) error (\( \delta_i \)) by minimizing:
\begin{align}
\mathcal L(\theta_i) &= \mathbb E_{(s_t, a_t) \sim \chi(\bullet)} 
    \big[ 
        (\underbrace{y_i - Q_{\theta_i}(s_t, a_t)}_{\delta_i})^2 
    \big], \label{eq:dqn-loss} \\
    y_i &= \mathbb E_{s_{t+1} \sim \mathbb P(\bullet \vert s_t, a_t)} \big[ r_t + \gamma \max_{\action \in \mathcal A} Q_{\theta_{i-1}} (\stateplusone, a_{t+1}) \big], \label{eq:TD-target}
\end{align}
Where \( \chi \) represents a behavior distribution over state-action pairs. 
Crucially, \( \chi \) can in principle be different from the policy being followed, effectively allowing to reuse prior data stored in a \emph{replay buffer} in the form of \( \sars \) transitions, sampled and used to form the TD-target \( y_i \) and TD-error \( \delta_i \) during training.

While effective in discrete action-space problems, DQN's application continous control problems is challenging, as in the case of high-capacity function approximators such as neural networks, solving \( \max_{a_t \in \mathcal A} Q_\theta(s_t, a_t) \) at each timestep is simply unfeasible due to the (1) continous nature of the action space (\( \actionspace \subset \mathbb R^n \) for some \( n \)) and (2) impossibility to express the find a cheap (ideally, closed-form) solution to \( Q_\theta \).
~\citet{silverDeterministicPolicyGradient2014} tackle this fundamental challenge by using a \emph{deterministic} function of the state \( s_t \) as policy, \( \mu_\theta(s_t) = a_t \) parametrized by \( \phi \), in which case policies can be iteratively refined by updating \( \phi \) along the direction:
\begin{equation}\label{eq:deterministic-pg}
    d_\phi = \mathbb E_{s_t \sim \mathbb P (\bullet)} \left[ \nabla_\phi Q(s_t, a_t)\vert_{a_t = \mu_\phi(s_t)} \right] = \mathbb E_{s_t \sim \mathbb P(\bullet)} \left[ \nabla_{a_t} Q(s_t, a_t) \vert_{a_t = \mu_\phi(s_t)} \cdot \nabla_\phi \mu(s_t) \right]
\end{equation}
Provably, \ref{eq:deterministic-pg} is the (deterministic) policy gradient of the policy \(\mu_\phi \)~\citep{silverDeterministicPolicyGradient2014}, so that updates \( \phi_{k+1}\leftarrow \phi_k + \alpha d_\phi \) are guaranteed to increase the (deterministic) cumulative discounted reward, \( J(\mu_\phi) \).
~\citet{lillicrapContinuousControlDeep2019} demonstrated the use of deep learning for problems dealing with (1) high-dimensional unstructured observations and (2) continuous action spaces, introducing Deep Deterministic Policy Gradient (DDPG), an important algorithm RL and its applications to robotics.
DDPG modifies the TD-target defined in~\ref{eq:TD-target} maintaining a policy network used to select actions, so that the TD-target becomes:
\begin{equation}\label{eq:TD-target-ddpg}
y_i = \mathbb E_{s_{t+1} \sim \mathbb P(\bullet \vert s_t, a_t)} \big[ r_t + \gamma Q_{\theta_{i-1}} (\stateplusone, \mu_\phi(\stateplusone)) \big] .
\end{equation}

Similarily to DQN, DDPG employs on the same replay buffer mechanism to increase sample efficiency, effectively reusing past transitions over training.
Soft Actor-Critic (SAC)~\citep{haarnojaSoftActorCriticOffPolicy2018} is a variant of DDPG in the max-entropy (MaxEnt) framework, in which RL agents are tasked with \highlight{maximizing the discounted cumulative reward, while acting as randomly as possible}.
MaxEnt RL~\citep{haarnojaReinforcementLearningDeep2017} has proven particularly robust thanks to the development of diverse behaviors, incentivized by its entropy-regularization formulation.
In that, MaxEnt revisits the RL objective \( J (\pi) \) to specifically account for the policy entropy, yielding:
\begin{align}
    J(\pi) &= \sum_{t=0}^T \mathbb{E}_{(s_t, a_t) \sim \chi} \left[ r_t + \alpha \mathcal H(\pi (\bullet \vert s_t)) \right] \label{eq:J-soft}
\end{align}
This modified objective results in the soft-TD target:
\begin{equation}\label{eq:soft-td-target}
    y_i = \mathbb E_{s_{t+1} \sim \mathbb P( \bullet \vert s_t, a_t)} \left[ r_t + \gamma \left( Q_{\theta_{i-1}} (\stateplusone, a_{t+1}) - \alpha \log \pi_\phi(a_{t+1} \vert \stateplusone) \right) \right], \quad a_{t+1} \sim \pi_\phi(\bullet \vert s_t)
\end{equation}
Similarily to DDPG, SAC maintains an explicit policy as well, trained under the same MaxEnt framework for the maximization of \ref{eq:J-soft}, updated using:
\begin{equation}\label{eq:sac-policy-update}
    \pi_{k+1} \leftarrow \arg\min_{\pi^\prime \in \Pi} \text{D}_{\text{KL}} \left(\pi^\prime (\bullet \vert \state) \bigg\Vert \frac{\exp(Q_{\pi_k}(s_t, \bullet))}{Z_{\pi_k}(s_t)} \right)
\end{equation}
The update rule provided in \ref{eq:sac-policy-update} optimizes the policy~\citep{haarnojaReinforcementLearningDeep2017} while projecting it on a set \( \Pi \) of tractable distributions (e.g., Gaussians).
Importantly, sampling \( \transition \) from the replay buffer \( D \) conveniently allows to approximate the previously introduced expectations for TD-target and TD-error through Monte-Carlo (MC) estimates.

% SAC + prior data: RLPD
The replay buffer \( D \) proves extremely useful in maintaining a history of previous transitions and using it for training, improving on sample efficiency.
Furthermore, it also naturally provides an entry point to store offline trajectories recorded, for instance, by a human demonstrator, and to use this prior data during training.
In Reinforcement Learning with Prior Data (RLPD),~\citet{ballEfficientOnlineReinforcement2023} derive an Offline-to-Online RL algorithm leveraging prior data to effectively accelerate the training of a SAC agent.
Unlike previous works on Offline-to-Online RL, RLPD avoids any pre-training and instead uses the available offline data \( D_\text{offline} \) to improve online-learning from scratch.
During each training loop, transitions from both the offline and online replay buffers are sampled in equal proportion, and used in the underlying SAC routine.

% RLPD + reward classifier: SERL
Despite the possibility to leverage offline for learning, the effectiveness of real-world RL training is still limited by the need to define a task-specific, hard-to-define reward function.
Further, even assuming to have access to a well-defined reward function, typical robotics pipelines rely mostly on propioperception, augmented by camera streams of the environment.
As such, even well-defined rewards would need to be derived from processed representations of unstructured observations, introducing brittleness.
In their technical report,~\citet{luoSERLSoftwareSuite2025} empirically address the needs (1) to define a reward function and (2) to use it from image observations, by introducing a series of tools to allow for streamlined training of \emph{reward classifiers} \( c \) as well as (3) jointly learn forward-backward controllers to speed up real-world RL.
Reward classifiers are particularly useful in treating complex tasks---e.g., folding a napkin---for which a precise reward formulation is arbitrarily complex to obtain, or that do require significant shaping and are more easily learned directly from demonstrations of successful (\(e^+\)) or failing (\(e^-\)) episodes for given \( s \in \statespace \), with a natural choice for the state-conditioned reward function \( r \mathcal S \mapsto \mathbb R \) in such cases being \( r(s) = \log c(e^+ \ vert s ) \).
Lastly,~\citep{luoSERLSoftwareSuite2025} do demonstrate the benefits of learning forward (executing the task from initial state to completion) and backward (resetting the environment to the initial state from completion) controllers, learned as separate policies.
[FROM HERE]
Lastly, in order to improve on the robustness of the approach used 

Importantly, SERL also presents

% SERL + Human in the loop: HIL-SERL

Building on off-policy deep Q-learning with replay buffers for sample-efficiency, entropy regularization for better exploration and performance, ingestion of prior data and expert demonstrations to guide learning, and a series of tools and recommendations for real-world training using reward classifiers,~\citet{luoPreciseDexterousRobotic2024} investigate the effects of allowing for human interactions over training, achieving near-optimal success rates while also 

Recent work proposed a practical pathway to real-world training without simulators via \emph{Human-in-the-Loop, Sample-Efficient RL} (HIL-SERL)~\citep{luoPreciseDexterousRobotic2024}.

At a high level, HIL-SERL augments online RL with human demonstrations and targeted human corrections. Demonstrations provide an initial dataset that seeds learning and constrains early exploration; interactive corrections allow a human supervisor to intervene on failure modes and supply short, high-signal trajectories that the agent incorporates immediately. Efficient off-policy updates reuse all collected data, compounding sample efficiency, while system-level safeguards maintain safety during training.

This design directly addresses the two central limitations above. Because data are collected on the real robot from the outset, there is no sim-to-real transfer gap. Because demonstrations and corrections bias exploration toward relevant behaviors, the agent reaches competent performance in hours rather than days, reducing both wear-and-tear and the need for exhaustive resets. Empirically, HIL-SERL attains near-perfect success rates on diverse manipulation tasks (dynamic manipulation, precision assembly, dual-arm coordination) within 1--2.5 hours of training~\citep{luoPreciseDexterousRobotic2024}. These results align with broader evidence that blending offline datasets with online RL can markedly improve stability and data efficiency~\citep{ballEfficientOnlineReinforcement2023}.

Within \lerobot, we adopt these principles to make real-world RL practical by design: emphasize data reuse, allow human-in-the-loop corrections, and keep the training loop grounded in real data rather than simulators. This yields policies that are both sample-efficient and robustly deployable on the target platform.

[TO HERE]
\subsubsection*{Code Example: Training RL Policies with \lerobot}
Outside of simulation environments, lerobot implements HIL-SERL for real-world robotics

coding example of how HIL-SERL works

\subsubsection{Limitations of RL in Real-World Robotics: Simulators and Reward Design}
[TO CHECK]
High-fidelity simulators for real-world robotics are difficult to build and maintain, especially for contact-rich manipulation and tasks involving deformable or soft materials. Accurate modeling of contact dynamics, frictional phenomena, compliance, and wear requires numerous hard-to-measure parameters and careful calibration. Moreover, reproducing sensing stacks (e.g., camera pipelines with rolling shutter, lens distortions, exposure control, latency) and closed-loop perception is non-trivial~\citep{bekrisStateRobotMotion2024,sicilianoSpringerHandbookRobotics2016}. While domain randomization can improve robustness to modeling errors~\citep{akkayaSolvingRubiksCube2019}, and large-scale sim training has delivered impressive locomotion and loco-manipulation capabilities~\citep{hwangboLearningAgileDynamic2019,leeLearningQuadrupedalLocomotion2020,jiDribbleBotDynamicLegged2023}, the engineering effort remains substantial and task-specific.

Reward design poses an additional source of brittleness. Dense shaping terms are often required to guide exploration in long-horizon problems, but poorly tuned terms can lead to specification gaming or local optima. Sparse rewards avoid shaping but exacerbate credit assignment and slow down learning. In practice, complex behaviors frequently require staged curricula or hierarchical decompositions to make rewards tractable~\citep{zhangWoCoCoLearningWholeBody2024,leeLearningQuadrupedalLocomotion2020}. Even then, changes in hardware, sensing, or environment can invalidate previously tuned rewards and necessitate time-consuming redesign~\citep{tangDeepReinforcementLearning2025a}.

These limitations collectively motivate data-driven real-world training pipelines that minimize dependency on high-fidelity simulators and reduce reward-engineering demands. Human-in-the-loop methods (e.g., HIL-SERL) and imitation-driven approaches provide precisely such a path: they leverage demonstrations and corrections to implicitly encode task structure, focus learning on relevant regions of the state space, and ultimately yield policies that transfer by construction because they are trained where they will be deployed.
