\section{Robot (Imitation) Learning}
\label{sec:learning-bc-single}

\epigraph{\emph{The best material model for a cat is another, or preferably the same cat}}{Norbert Wiener}

Learning from human demonstrations provides a pragmatic alternative to the reinforcement-learning pipeline discussed in Section~\ref{sec:learning-rl}.
In real-world robotics, online exploration is typically \highlight{costly and potentially unsafe}, and (dense) reward design is \highlight{brittle and task-specific}.
In general, success detection often may require bespoke instrumentation, and episodic training demands reliable resets---all factors that complicate training RL algorithms on hardware at scale.
Behavioral Cloning (BC) sidesteps these constraints by casting control an imitation learning problem, leveraging previously collected expert demonstrations.
In this way, the objectives, preferences, and success criteria are implicitly encoded in the data, obviating hand-crafted reward shaping and reducing early-stage exploratory failures.

Formally, let \( \mathcal D = \{ \tau^{(i)} \}_{i=1}^N \) be a set of expert trajectories, with \( \tau^{(i)} = \{(o_t^{(i)}, a_t^{(i)})\}_{t=0}^{T_i} \), where \(o_t \in \obsspace \) denotes observations (e.g., images and proprioception), and \(a_t \in \actionspace \) the expert actions.
Figure~\ref{fig:ch4-bc-trajectories} shows observations over sample trajectories for a dataset collected by a human demonstrating a manipulation task on the SO-100 arm, while Figure~\ref{fig:ch4-observation-action-mapping} shows \( (o_t, a_t) \)-pairs for the same dataset, with the actions commanded by the human expert illustrated alongside the corresponding observation.
Note that differently from Section~\ref{sec:learning-rl}, in this context \( \mathcal D \) indicates an offline dataset (and not the dynamics), collecting \( N \) length-\( T_i \) \emph{reward-free} (expert) human trajectories \( \tau^{(i)} \) (and not the a trajectory for a given MDP).
In principle, (expert) trajectories \( \tau^{(i)} \) can have different lengths since demonstrations might exhibit multi-modal strategies to attain the same goal, resulting in possibly multiple, different behaviors.

Behavioral Cloning (BC)~\citep{pomerleauALVINNAutonomousLand1988a} aims at synthetizing these behaviors learning the mapping from observations to actions, and in its simplest formulation can be effectively be tackled as a \emph{supevised} learning problem, where one learns a mapping \( a_t = \mu(o_t) \) by solving
\begin{equation}\label{eq:loss-minimization-SL}
    \min_{\mu} \mathbb{E}_{(o_t, a_t) \sim \mathcal{D}^*} \mathcal L(a_t, \mu(o_t)),
\end{equation}
for a given risk function \( \mathcal L:  \mathcal A \times \mathcal A \mapsto \mathbb{R}, \ \mathcal L (a, a^\prime) \).

The expert's joint observation-action distribution \( \mathcal D^* \) is typically assumed to be unknown, similarily to a typical Supervised Learning (SL) framework\footnote{Throughout, we will adopt terminology and notation for SL in keeping with \citet{shalev-shwartzUnderstandingMachineLearning2014}}.
However, differently from standard SL's assumptions, the samples collected in \( \mathcal D \) are \emph{not} typically i.i.d., as expert demonstrations are collected \emph{sequentially} in trajectories.
In practice, this aspect can be partially mitigated by considering pairs in a non-sequential order---\emph{shuffling} the samples in \(\mathcal D \)---so that the expected risk under \( \mathcal D^* \) can be estimated using MC estimates, drawing \((o_t, a_t)\) from \( \mathcal D \), although estimates can in general be less accurate.
Another strategy to mitigate the impact of having non-i.i.d. samples relies on the possibility of interleaving BC and data collection~\citep{rossReductionImitationLearning2011}. 
However, because we consider the case where a single offline dataset \( \mathcal D \) dataset of (expert) trajectories has been entirely collected already, this more advanced method is typically not applicable to the considered setup.

Despite challenges, the BC formulation affords several operational advantages in robotics.
First, training happens offline and typically uses expert human demonstration data, hereby severily limiting risks.
Second, reward design is unnecessary in BC, as demonstrations already reflect human intent and task completion, mitigating misalignment and the risk of specification gaming (\emph{reward hacking}) otherwise inherent in purely reward-based RL~\citep{heessEmergenceLocomotionBehaviours2017}.
Third, because expert trajectories encode terminal conditions, success detection and resets are implicit in the dataset.
Finally, BC scales naturally with growing corpora of demonstrations collected across tasks, embodiments, and environments.
Crucially, BC can in principle only learn behaviors that are, at most, as good as the one exhibited by the demonstrator.
While problematic in sequential-decision making problems for which expert demonstrations are not generally available---data migth be expensive to collect, or human performance inherently suboptimal---many application of robotics benefit from relative cheap pipelines to acquire high-quality trajectories generated by humans. 

However, point-estimate policies \( \mu : \obsspace \mapsto \actionspace \) learned by solving \ref{eq:loss-minimization-SL} have been observed to suffer from (1) compounding errors~\citep{rossReductionImitationLearning2011} and (2) poor fit to multimodal distributions~\citep{florenceImplicitBehavioralCloning2022, keGraspingChopsticksCombating2020} (Figure~\ref{fig:ch4-issues-with-bc}).
Besides sequentiality in \( \mathcal D \), compounding errors are due to \emph{covariate shift}, whereby even small \( \epsilon \)-prediction errors \( 0 < \Vert \mu(o_t) - a_t \Vert \leq \epsilon \) can quickly drive the policy into out-of-distribution states, incuring in less confident generations and thus errors compounding (Figure~\ref{fig:ch4-issues-with-bc}, left).
Further, point-estimate policies typically fail to learn \emph{multimodal} (in the stastical sense) targets, common in robotics problems where multiple trajectories can be equally as good towards the realization of a given objective (e.g.,  symmetric grasps, Figure~\ref{fig:ch4-issues-with-bc}, left). 
In particular, unimodal regressors tend to average across modes, yielding indecisive or even unsafe commands~\citep{florenceImplicitBehavioralCloning2022}.
To address poor multimodal fitting,~\citet{florenceImplicitBehavioralCloning2022} propose learning the generative model \( p(o, a) \) underlying the samples in \( \mathcal D \), rather than an explicit prediction function \( \mu(o) = a \).

\subsection{A (Concise) Introduction to Generative Models}
% Generative Modeling
Generative Models (GMs) aims to learn the (stochastic) process underlying the very generation of the data collected, and it typically does so by fitting a probability distribution that approximates the unknown \emph{data distribution}, \( \mathcal D^* \).
In the case of BC, this unknown data distribution \( \mathcal D^* \) represents the expert's joint distribution over \( (o, a) \)-pairs, where \( (o,a) \sim \mathcal D^* \).
Thus, given a finite set of \( N \) pairs \(\mathcal D = \{ (o,a)_i \}_{i=0}^N\) used as an imitation learning target, GM seeks to learn a \emph{parametric} distribution \( p_\theta(o,a) \) such that (1) samples \( (o,a) \sim p_\theta(\bullet) \) resemble the input data \( \mathcal D \), and (2) high likelihood is assigned to observed regions of the unobservable \( p \).
Likelihood-based learning provides a principled training objective to achieve both objectives, and it is thus extensively used in GM~\citep{prince2023understanding}.

% VAEs
\paragraph{Variational Auto-Encoders}
A common inductive bias used in GM posits samples \( (o,a) \) arise from an unobservable latent variable \( z \in Z \), resulting in
\begin{equation}\label{eq:BC-latent-variable}
    p (o,a) = \int_{\supp{Z}} p(o,a \vert z) p(z)
\end{equation}
Intuitively, in the case of observation-action pairs \( (o, a) \), \( z \) could perhaps contain some sort of representation of the underlying task being performed by the human demonstrator.
In such case, treating \( p(o,a) \) as a marginalization over \( \supp{Z} \) of the complete joint distribution \( p(o,a,z) \) natively captures the effect different tasks have on observation-action pairs.
Figure~\ref{fig:ch4-task-effect-on-pairs} graphically illustrates this concept in the case of a (A) picking and (B) pushing task, for which, nearing the target object, the likelihood of actions resulting in opening the gripper---the higher \( q_6 \), the wider the gripper's opening---should intuitively be (A) high or (B) low, depending on the task performed.
While the latent space \( Z \) typically has a much richer structure than the set of all actual tasks performed,~\ref{eq:BC-latent-variable} still provides a solid framework to learn joint distribution conditioned on unobservable yet relevant factors.
Figure~\ref{fig:ch4-latent-variable-model} represents this framework of latent-variable for a robotics application: the true, \( z \)-conditioned generative process on assigns \emph{likelihood} \( p_\theta(\bullet \vert z) \) to the single \( (o,a) \)-pair, for a given parametric distribution \(p_\theta \) (e.g., Gaussian).
Using Bayes' theorem, we could even reconstruct the \emph{posterior} distribution on \( \supp{Z} \), \( p_\theta(z \vert o,a) \) from the likelihood \( p_\theta(o,a \vert z) \), \emph{prior} \( p_\theta(z) \) and \emph{evidence} \( p_\theta(o,a) \).

By a similar argument, given a dataset \( \mathcal D \) consisting of \( N \) i.i.d. observation-action pairs, the log-likelihood of all datapoints under \( \theta \) (in Bayesian terms, the \emph{evidence} \( p_\theta(\mathcal D)\)) can be written as:
\begin{align}
    \log p_\theta(\mathcal D) &= \log \sum_{i=0}^N p_\theta ((o,a)_i) \label{eq:evidence-definition-1}\\
                              &= \log \sum_{i=0}^N \int_{\supp{Z}} p_\theta((o,a)_i \vert z) p(z) \label{eq:evidence-definition-2}\\
                              &= \log \sum_{i=0}^N \int_{\supp{Z}} \frac{p_\theta(z \vert (o,a)_i)}{p_\theta(z \vert (o,a)_i)} \cdot p_\theta((o,a)_i \vert z) p(z) \label{eq:evidence-definition-3}\\
                              &= \log \sum_{i=0}^N \mathbb E_{z \sim p_\theta(\bullet \vert (o,a)_i)} \left[ \frac{p(z)}{p_\theta(z \vert (o,a)_i)} \cdot p_\theta((o,a)_i \vert z) \right], \label{eq:evidence-definition}
\end{align}
where we used~\ref{eq:BC-latent-variable} in~\ref{eq:evidence-definition-1}, multiplied by \(1 = \frac{p_\theta(z \vert (o,a)_i)}{p_\theta(z \vert (o,a)_i)} \) in~\ref{eq:evidence-definition-2}, and used the definition of expected value in~\ref{eq:evidence-definition}.

In the special case where one assumes distributions to be tractable, \( p_\theta (\mathcal D) \) is typically tractable too, and \(\max_\theta \log p_\theta(\mathcal D) \) provides a natural target for (point-wise) infering the unknown parameters \( \theta \) of the generative model.
Unfortunately, tractability of~\ref{eq:evidence-definition} is rarely induced by using distributions \( p \) modeled using approximators, like neural networks, to model high-dimensional, unstructured data.

In their seminal work on Variational Auto-Encoders (VAEs),~\citet{kingmaAutoEncodingVariationalBayes2022} propose two major contributions to learn complex latent-variable GMs on unstructured data: (1) propose a tractable, variational lower-bound to \ref{eq:evidence-definition} as an optimization target to learn \( \Phi \) and (2) use high-capacity function approximators to model the likelihood \(p_\theta(o,a\vert z)\) and (approximate) posterior distribution \( q_\phi(z \vert o,a) \approx p_\theta(z \vert o,a) \).

In particular, the lower bound on~\ref{eq:evidence-definition} (Evidence LOwer Bound, \emph{ELBO}) can be derived applying Jensen's inequality---\(\log \mathbb{E}[\bullet] \geq \mathbb{E} [\log (\bullet)] \)---to~\ref{eq:evidence-definition}, yielding:
\begin{align}
    \log p_\theta(\mathcal D) &\geq \sum_{i=0}^{N} \left(
            \mathbb{E}_{z \sim p_\theta(\cdot \vert (o,a)_i)} \big[ \log p_\theta((o,a)_i \vert z) \big]
            + \mathbb{E}_{z \sim p_\theta(\cdot \vert (o,a)_i)} \left[ \log \left( \frac{p(z)}{p_\theta(z \vert (o,a)_i)} \right) \right]
        \right) \\
        &= \sum_{i=0}^{N} \left(
            \mathbb{E}_{z \sim p_\theta(\cdot \vert (o,a)_i)} \big[ \log p_\theta((o,a)_i \vert z) \big]
        - \DKL \big[ p_\theta(z \vert (o,a)_i) \Vert p(z) \big]
        \right) \label{eq:ELBO-intractable}
\end{align}
The true, generally intractable posterior \( p_\theta (z \ vert o,a) \) prevents computing both the expectation and KL divergence terms in~\ref{eq:ELBO-intractable}.
Crucially,~\citet{kingmaAutoEncodingVariationalBayes2022} propose deriving the ELBO using an \emph{approximate} posterior \( q_\phi(z \vert o,a) \), resulting in the final, tractable ELBO objective,
\begin{align}
\text{ELBO}_{\mathcal D}(\theta, \phi) = \sum_{i=0}^{N} \left(
            \mathbb{E}_{z \sim q_\phi(\cdot \vert (o,a)_i)} \big[ \log p_\theta((o,a)_i \vert z) \big]
        - \DKL \big[ q_\phi(z \vert (o,a)_i) \Vert p(z) \big]
        \right)
        \label{eq:ELBO}
\end{align}
From Jensen's inequality, maximizing ELBO results in maximizing the log-likelihood of the data too, thus providing a natural, tractable optimization target.
Indeed, expectations can be estimated using MC estimates from the learned distributions in~\ref{eq:ELBO}, while the KL-divergence term can typically be computed in closed-form (1) modeling  \( \) and (2) 
A particularly intuitive explanation of learning GMs optimizing ELBO can be given considering minimizing the \emph{negative ELBO}, i.e. \( \min_{\theta, \phi} - \text{ELBO}_{\mathcal D}(\theta, \phi) = \min_{\theta, \phi}\mathbf{L^{\text{rec}}}(\theta) + \mathbf{L^{\text{reg}}}(\phi) \).

For any given \((o,a) \) pair, the expected value term of~\ref{eq:ELBO} is typically computed via MC estimates, resulting in
\[ 
-\mathbb{E}_{z \sim q_\phi(\bullet \vert o,a)} \big[ \log p_\theta(o,a \vert z) \big] = \mathbf{L^{\text{rec}}} \approx - \frac{1}{n} \sum_{i=0}^n \log p_\theta(o,a \vert z_i).
\]
Assuming \( p_\theta(o,a \vert z) \) is parametrized as an isotropic Gaussian distribution with mean \(\mu_\theta (z) \in \mathbb R^d \) and variance \( \sigma^2 \), the log-likelihood simplifies to:
\[
\log p(o,a \vert z_i) = -\frac{1}{2\sigma^{2}} \big \Vert (o,a)-\mu_\theta(z_i) \big\Vert_2^2 -\frac{d}{2}\log(2\pi \sigma^{2}) \implies \mathbf{L^\text{rec}} \approx \frac {1}{n} \sum_{i=0}^n \big\Vert (o,a) - \mu_\theta(z_i) \big \Vert^2_2
\]
In practice, samples from the learned likelihood \( p_\theta(o,a \vert z) \) are often collected drawing from parametric distributions (e.g. Gaussians) parametrized by some vector of coefficients derived from \(\mu_\theta (z), \ z \sim p_Z (\bullet) \).
In such cases, learning a GM partly corresponds to \emph{reconstructing} the examples in \( \mathcal D \), by minimizing the L2-error---a very common \emph{supervised learning} objective for continuous targets.
Further, the second term in \( -\text{ELBO}_\mathcal D(\theta, \phi) \)---\( \mathbf{\mathbf{L^{\text{reg}}}(\phi)} \)---results in the regularization of the approximate posterior, under the common modeling choice \( p(z) \sim N(\mathbf{0}, \mathbf{I}) \), which limits the expressivity of \( q_\phi(z\vert o,a) \).

% Diffusion
\paragraph{Diffusion Models}
VAEs approximate probability distributions via a \emph{single} latent variable model, assuming the underlying unknown distribution samples are obtained from can be factored according to~\ref{eq:BC-latent-variable}, and solve the variational inference problem of jointly learning the likelihood \( p_\theta \) and (approximate) posterior \( q_\phi \) for such model.
In that, the unknown data distribution \( p(o,a) \) is effectively approximated via \( \int_Z p(z) p_\theta(o,a \vert z) \), and the underlying generative process reproduced by (1) sampling a latent variable and (2) learning to decode it into a (ideally) high-likelihood sample under the (unknown) \( p(o,a) \).
Diffusion Models (DMs)~\citep{hoDenoisingDiffusionProbabilistic2020} are another class of GMs which treat the similar problem of approximating an underlying unknown data distribution---\emph{variational inference}---by partially extending VAEs to the case where \emph{multiple} latent variables influence each other and the generative process underlying \(o,a\) itself.
In particular, DMs posit the generative process can be decomposed to a series of piece-wise (Markovian) interactions between (latent) variables (Figure~\ref{fig:ch4-diffusion}), resulting in
\begin{align}
    p(\underbrace{o,a}_{= z_0}) &= \int_{\supp{Z_0}} \int_{\supp{Z_1}} \hdots \int_{\supp{Z_T}} p(z_0, z_1, \dots z_T) \label{eq:BC-multi-latent-model-1} \\ 
    &= p(z_T) \prod_{t=0}^{T} p(z_{t-1} \vert z_t), \label{eq:BC-multi-latent-model-2}
\end{align}
where we explicitly showed the marginalization over the multiple latents in~\ref{eq:BC-multi-latent-model-1}, and used the law of conditional probability and Markov property in~\ref{eq:BC-multi-latent-model-2}.
Similarily to VAEs, providing an exact interpretation for the latent variables is typically not possible.
Still, one fairly reasonable application-driven intuition is that, by providing a model of the hierarchical, decoupled interaction of latent variables, DMs attempt to capture the different resolutions at which different conditioning intervene, so that, in a robotics application, one can naturally distinguish between early-stage trajectory planning (large \( t \)) and fine-grained adjustments (small \( t \)).
DMs thus provide a framework to perform variational inference via multiple, sequential sampling steps from different higher level distributions instead of approximating the generative process with a single-latent variable model.

Just like VAEs, DMs attemp to learn to reproduce an underlying data distribution \( p (o,a) \) given a collection of i.i.d. samples by means of approximating the model (~\ref{eq:BC-multi-latent-model-1}) posited to have generated the data in the first place.
Similarily to VAEs, DMs approximate the process of sampling from the unknown \( p(o,a) \) (1) sampling from an easy-to-sample distribution (e.g., Gaussian) and (2) learning to reconstruct high-likelihood samples under the unknown distribution, while in stark contrast with VAEs the easy-to-sample distribution contains \emph{no mutual information} regarding the data distribution \( p(o,a) \).
Crucially, as no information from the sample \( (o,a) \) (denoted as \( z_0 \equiv (o,a) \) for the sake of notation) is assumed to be propagated throughout the chain of latents, the \emph{approximation} \( q(z_t \vert z_{t-1} ) \) of the \emph{true} posterior \( p(z_t \vert z_{t-1})\) can assume a relatively easy structure, reducing complexity.
The \emph{true} likelihood \( p(z_{t-1} \vert z_t) \) is instead typically approximated using the aforementioned parametrization, \(  p_\theta (z_{t-1} \vert z_t) \).
In that, the information contained in the unknwon data distribution is \emph{reconstructed} via a process in which samples from a fixed distribution are turned into (ideally) high-likelihood samples under \( p(o,a) \) (\emph{denoising}).

Under such model, we can express the log-likelihood of the \emph{denoised} sample as\footnote{Steps omitted for brevity. See Section A in~\citet{hoDenoisingDiffusionProbabilistic2020} for a complete derivation.
% See~\ref{appendix:diffusion-derivation}
}

\begin{align}
    \log p_\theta (\underbrace{o,a}_{= z_0}) &= \mathbb{E}_{z_1 \sim q(\bullet \vert z_0)} \log p_\theta (z_0 \vert z_1) \label{eq:diffusion-likelihood} \\
    &- \mathbb{E}_{z_{T-1} \sim q(\bullet \vert z_0)} \big[ \DKL (q(z_T \vert z_{T-1}) \Vert p(z_T) ) \big] \notag \\ 
    &- \sum_{t=1}^{T-1} \mathbb{E}_{(z_{t-1}, z_{t+1}) \sim q(\bullet \vert z_0)} \big[ \DKL (q(z_t \vert z_{t-1}) \Vert p_\theta(z_t \vert z_{t-1}) ) \big] \notag
\end{align}
providing an optimization target in the form of maximizing \( \log p_\theta(o,a) \) over \( \theta \) and the considered \emph{hierarchical Markov latent-variable model}.

In their seminal work on using DMs for variational inference,~\citet{hoDenoisingDiffusionProbabilistic2020} introduce major contributions regarding solving \( \min_\theta \log p_\theta(o,a) \).
In particular,~\citet{hoDenoisingDiffusionProbabilistic2020} exclusively adopt a fixed \emph{Gaussian} posterior, and in particular use \( q(z_t \vert z_{t-1}) \sim N(\sqrt{1-\beta_t}z_{t-1}, \beta_t \mathbf I) \).
The choice of adopting Gaussians has profound implications on the generative process modeled itself because, under the (mild) assumption on the variance of the posterior being small \( \beta_t \leq \eta, \eta \in \mathbb R^+ \), then the \emph{true} likelihood \( p(z_{t-1} \vert z_t) \) is Gaussian too~\citep{sohl-dicksteinDeepUnsupervisedLearning2015}, allowing for the particularly convenient parametrization of the approximate likelihood---\( p_\theta (x_{t-1} \vert x_t) = N(\mu_\theta(x_t, t), \Sigma_\theta(x_t,t)), \ t \in [1,T] \)---and for closed-form tractability of the KL-divergence terms in~\ref{eq:diffusion-likelihood}.
Further, the posterior's structure also admits an analytical description of the general distribution of the \( t\)-th latent variable, \( q(z_t \vert z_0) = N (\sqrt{\bar{\alpha}_t}z_0, (1-\bar{\alpha}_t) \mathbf{I}) \), with \( \alpha_t = 1-\beta_t, \ \bar \alpha_t = \prod_{k=1}^t \alpha_k \).
~\citet{hoDenoisingDiffusionProbabilistic2020} also show they 

Further, adopting Gaussian posteriors permits a particularly pleasing interpretation of the dynamics of training DMs~\citep{permenterInterpretingImprovingDiffusion2024}.
By using Gaussian posteriors, the hierarchical latent variables effectively lose increasingly more information circa the original (unknown) distribution's sample, \( z_0 \), eventually distributing according to a standard Gaussian and thus containing no information at all (Figure~\ref{fig:diffusion-robot-actions}).
Figure~\ref{fig:diffusion-robot-actions} illustrates this procedure on a simplified, bidimensional observation-action distribution, where we considered \( o=q_2\) and \( a=q^h_2\), with \( q_2 \) representing the robot's \emph{elbow flex} actuation and \( q^h_2 \) the human teleoperator's robot elbow flex. 
Because the recorded behavior is teleoperated, measurements mostly distribute along the line \( a = o + \eta, \eta \sim N(0,1) \), with \( \eta \)-variability accouting for minor control inconsistencies.
Using Gaussian posteriors---i.e., adding Gaussian noise---effectively simulates a Brownian motion for the elements in the distribution's support (in Figure~\ref{fig:diffusion-robot-actions}, \( \obsspace \times \actionspace \)), whereby information \emph{diffuses away} from the samples, and compare the diffused samples to the original data points, deriving an estimate of the total displacement induced by diffusion.
Under the only---extremely reasonable---assumption that the likelihood of the diffused samples is low under the original unknown data distribution, then we can effectively approximate the unkwown distribution by learning to reverse such displacement.
This key intuition allows to write a simplified training objective, taking benefit of the Brownian motion interpretation of diffusion, defined as~\citet{hoDenoisingDiffusionProbabilistic2020}:
\begin{align}\label{eq:diffusion-simplified-loss}
    \mathcal L(\theta) = \mathbb{E}_{t, z_0, \epsilon} \big[
        \Vert \epsilon - \epsilon_\theta(\sqrt{\bar \alpha_t} z_0 + \epsilon \sqrt{1 - \bar \alpha_t}, t) \Vert^2 \big], \quad t \sim \mathcal{U}(\{1,\dots,T\}) \quad
        z_0 \sim \mathcal{D} \quad
        \epsilon \sim \mathcal{N}(\mathbf{0},\mathbf{I})
\end{align}
In this simplified (minimization) objective, the learning process differs from~\ref{eq:diffusion-likelihood} in that, rather than maxizing \( p_\theta \) directly, parameters are adjusted by learning to \emph{predict the total displacement} \( \epsilon \) for a randomly long (\( t \sim \mathcal{U}(\{1,\dots,T\} \))) diffusion process starting from a random sample of the target distribution.
By learning the total displacement from a generally, uninformative corrupted sample obtained diffusing information and a sample from an unknown distribution---significant (\( \Vert \epsilon \Vert > 0 \)) whenever input and target distribution are sufficiently different---~\citet{hoDenoisingDiffusionProbabilistic2020} show that one can effectively learn the underlying distribution.
Interestingly, under the hypothesis real-world data belongs to a single higher dimensional manifold (Manifold Hypothesis),~\citet{permenterInterpretingImprovingDiffusion2024} show diffusion learns the gradient of a distance function from any off-point manifold, such as perturbed samples, and the data manifold. 
Following this gradient---i.e., \emph{denoising} a sample from an uninformative distribution---then equates to projecting back into the manifold, yielding a procedure to sample from unknown distributions by means of Euclidean projection.



\subsection{Action Chunking with Transformers}
While GMs prove useful in learning complex, high-dimensional multi-modal distributions, they do not natively address the compouding errors problem characteristic of online, sequential predictions.
In Action Chunking with Transformers (ACT),~\citet{zhaoLearningFineGrainedBimanual2023} present an application of VAEs to the problem of learning purely from offline trajectories, introduce a simple, yet effective method to mitigate error compounding, learning high-fidelity autonomous behaviors.
Drawing inspiration from how humans plan to enact atomically sequences of the kind \( a_{t:t+k} \) instead of single actions \( a_t \),~\citet{zhaoLearningFineGrainedBimanual2023} propose learning a GM on a dataset of input demonstrations by modeling \emph{action chunks}.
Besides contributions to learning high-performance autonomous behaviors,~\citet{zhaoLearningFineGrainedBimanual2023} also introduce hardware contributions in the form of a low-cost bimanual robot setup (ALOHA) capable of performing fine-grained manipulation tasks, such as opening a lid, slotting a battery in its allotment or even prepare tape for application.

On the robot learning side of their contributions,~\citet{zhaoLearningFineGrainedBimanual2023} adopt transformers as the architectural backbone to learn a \emph{Conditional} VAE~\citep{sohnLearningStructuredOutput2015}. 
Conditional VAEs are a variation of the more standard VAE formulation introducing a conditioning variable on sampling from the latent prior, allowing the modeling of \emph{one-to-many} relationships between latent and data samples.
Further, in stark contrast with previous work~\citep{florenceImplicitBehavioralCloning2022,jannerPlanningDiffusionFlexible2022},~\citet{zhaoLearningFineGrainedBimanual2023} do not learn a full joint \( p_\theta(o,a) \) on observation and actions.
While the \emph{policy} distribution \( p_\theta(a \vert o) \) can in principle be entirely described from its joint \( p_\theta(o,a) \), it is often the case that the conditional distribution is intractable when using function approximators, as \( p_\theta(a \vert o) = \tfrac{p_\theta(o,a)}{\int_\actionspace p_\theta(o,a)} \) and the integral in the denominator is typically intractable.
Instead of modeling the full joint using a vanilla VAE,~\citet{zhaoLearningFineGrainedBimanual2023} propose learning a \emph{conditional} VAE modeling the policy distribution directly \( p (a \vert o) \).

In practice, when learning from demonstrations adopting CVAEs results in a slight modification to the VAE objective in~\ref{eq:ELBO}, which is adapted to
\begin{align}\label{eq:c-ELBO}
    \text{ELBO}_{\mathcal D}(\theta, \phi, \omega) = \sum_{i=0}^{N} \left(
            \mathbb{E}_{z \sim q_\phi(\cdot \vert o_i, a_i)} \big[ \log p_\theta(a_i \vert z, o_i) \big]
        - \DKL \big[ q_\phi(z \vert o_i, a_i) \Vert p_\omega(z \vert o_i) \big]
        \right)
\end{align}
Notice how in~\ref{eq:c-ELBO} we are now also learning a new set of parameters \( \omega \) for the prior distribution in the latent space.
Effectively, this enables conditioning latent-space sampling (and thus reconstruction) during training, and potentially inference, providing useful when learning inherently conditional distributions like policies.
Further, ACT is trained as a \( \beta\)-CVAE~\citep{higgins2017beta}, using a weight of the KL regularization term in~\ref{eq:c-ELBO} as an hyperparameter regulating the information condensed in the latent space, where higher \( \beta \) results in a less expressive latent space.

In their work,~\citet{zhaoLearningFineGrainedBimanual2023} ablated using a GM to learn from human demonstrations compared to a simpler, supervised objective, \( \mathcal L_1(a,a^\prime) = \Vert a - a^\prime \Vert_1 \).
Interestingly, they found the performance of these two approaches to be comparable when learning from \emph{scripted} demonstrations.
That is, when learning from data collected rolling out a predetermined set of commands \( [q^c_0, q^c_1, \dots] \), GM did \emph{not} prove competitive compared to standard supervised learning.
However, when learning from human demonstrations---i.e., from data collected executing commands coming from a human controller \( [q^h_0, q^h_1, \dots] \)---they found performance (success rate on a downstream task) to be severily (-33.3\%) hindered from adopting a standard supervised learning objective compared to a richer, potentially more complex to learn variational objective, in keeping with the multimodal nature of human demonstrations data and findings presented in~\citet{florenceImplicitBehavioralCloning2022}.
The authors also ablate the action chunking paradigm, reporting significant performance gains for performing action chunking (1\% vs. 44\% success rate).
To avoid acting openloop,~\citet{zhaoLearningFineGrainedBimanual2023} design an inference process consisting in performing inference at every timestep \( t \) and then aggregate overlapping chunks using chunks' exponential moving average.  

In ACT (Figure~\ref{fig:ch4-act}), inference for a given observation \( o \in \mathcal O \) could be performed by (1) computing a prior \( p_\omega(z \vert o) \) for the latent and (2) decoding an action chunk from a sampled latent \( z \sim p_\omega(\bullet \vert o) \), similarily to how standard VAEs generate samples, with the exception that vanilla VAEs typically pose \( p(z\vert o) \equiv p(z) \sim N(\mathbf{0}, \mathbf{I}) \) and thus skip (1).
However, the authors claim using a deterministic procedure to derive \( z \) may benefit policy evaluation, and thus avoid sampling from the conditional prior at all.
At test time, instead, they simply use \( z = \mathbf{0} \), as the conditional prior on \( z \) used in training is set to be the unit Gaussian.
At test time, conditioning on the observation \( o \) is instead achieved through explicitly feeding proprioperceptive and visual observations to the decoder, \( p_\theta(a \vert z, o) \), while during training \( z \) is indeed sampled from the approximate posterior distribution \(p_\phi(z \vert o, a)\), which, however, disregards image observations and exclusively uses proprioperceptive states to form \( o \) for efficiency reasons (as the posterior encoder \(q_\phi \) is completely disregarded at training time).

\subsubsection{Code Example: Learning ACT}
\todo{using act example}

\subsection{Diffusion Policy}
fill later

\subsubsection{Code Example: Learning Diffusion Policies}
\todo{using diffusion policy example}

\subsection{Optimized Inference via Decoupling Action Prediction and Execution}
Implicit World Modeling 
Async inference tightens control loop
Analyzing async inference

\subsubsection{Code Example: Deploy Optimized Policies}
\todo{async inference example}